# {{PROJECT_NAME}}

This project contains the application known as "{{PROJECT_NAME}}".
The interface is a REST API. It requires Python {{PYTHON_VERSION}}.

## Run the application

### Clone the application
Clone the application directly from the source code:
```
git clone {{GITHUB_REPO}}
cd "$(basename "{{GITHUB_REPO}}" .git)"
```

### Prerequisites

#### Create _{{PROJECT_ID_DEV}}_ and _{{PROJECT_ID_PROD}}_ Google projects
You have to create _{{PROJECT_ID_DEV}}_ and _{{PROJECT_ID_PROD}}_ Google projects and enable in them the following APIs:
* Cloud Run 
* Container Registry
* Resource Manager

#### Create a Service Account on _{{PROJECT_ID_DEV}}_ and _{{PROJECT_ID_PROD}}_ Google projects
You need to create a service account on _{{PROJECT_ID_DEV}}_ and _{{PROJECT_ID_PROD}}_ Google projects with the following permissions:
* Cloud Run Admin
* Service Account User
* Storage Admin
* Viewer
You will also need to create a json key for both service account, download them to your local system.

### Set environment variables
You need to set a few environment variables in your local environment:
```
PROJECT_DIR=PATH_TO_THIS_REPOSITORY
```
This variable contain the path that points to the repository root. The corresponding value is also used in CI/CD workflows
```
ENV=dev OR prod
```
This variable defines the environment. It can be set to either "dev" or "prod" and will be used locally to define the 
configuration file to use.

### Setup Circleci
To be able to run the Circleci workflows of this project you need to:
1. go to the [CircleCI web app](https://app.circleci.com/), select the `Projects` section from the left hand menu, search for {{PROJECT_NAME}} 
in the list and click on the "Set Up" button associated to it, following the subsequent instructions.
2. save the content of the key files of the two service accounts as CircleCI environment variables called respectively 
`GCLOUD_SERVICE_KEY_DEV` and `GCLOUD_SERVICE_KEY_PROD` by navigating to _Project Settings > Environment Variables_ in 
the [CircleCI web app](https://app.circleci.com/).
2. go to the [CircleCI web app](https://app.circleci.com/) and make sure to activate 
_Project Settings > Advanced > Enable dynamic config using setup workflows_.
3. in case you have dependecies from other private repositories:
   * go to the [CircleCI web app](https://app.circleci.com/) and make sure to activate _Project Settings > SSH Keys > User Key_ while having the browser console open.
   * once you activated the User Key you will get the json response of a `checkout-key` request in the `network` tab of the browser console
   * copy the value of `public-key` claim of that json response (it is a public ssh key)
   * paste it between the user ssh keys of your account (In Bitbucket: Log > Personal Bitbucket Settings > SSH Keys > Add key)

The first step is required for the CI/CD workflows to be activated, the second one to be able to access Google resources from Circleci workflows,
while the third one is required to be able to use multiple configuration files for CircleCI itself.   

### Start the webserver locally
Just use `make run` to run the application in a container: it will be available at address 127.0.0.1:8080.
On the other hand, to run the application locally, without a container, the user can run:
```
python -m app.main --host 127.0.0.1 --port 8080
```
And the application will be available at address 127.0.0.1:8080.

## Repository structure
After initialization repository is structured according to the following schema
```
PROJECT_DIR
│   README.md
│   Dockerfile      ==>  list of docker instructions to build imge
│   Makefile        ==>  list of make commands
│   LICENESE        ==>  project license
│   pyproject.toml  ==>  project configurations
│   .giattributes   ==>  file that gives attributes to pathnames
│   .gitignore      ==>  list of patterns to ignore with git
│   .dockerignore   ==>  list of patterns to ignore while building docker image
│   .commitlintrc.yaml ==>  configurations for commitlint
│   .pre-commit-config.yaml ==>  configurations for git hooks
│
├───.circleci
│   └──   config.yml  ==> CI and CD workflows with circleci
│
├───.github
│   │   RELEASE-TEMPLATE.md  ==> Template file for release notes, to be updated by realease action
│   │
│   └───workflows
│       │   continous-delivery.yml  ==> CD workflow
│       │   continous-integration.yml  ==> CI workflow
│       └── github-page-build-and-deploy.yml  ==> build and deploy github page with code documentation
│
├───app
│   │   __init__.py  ==> file containing version number
│   │   main.py      ==> application access point
│   │   config.py    ==> configuration classes
│   │   facade.py    ==> facade class
│   │   py.typed     ==> empty file required by mypy to recognize a typed package
│   │
│   └───core
│       │   __init__.py
│       │
│       ├───adapters  ==> configurations module
│       │
│       ├───application
│       │   ├───api ==> utilities module
│       │   │   │   __init__.py 
│       │   │   │   handlers.py ==> module containing base exception handlers.
│       │   │   └── routes.py   ==> module containing the implementation of the Routes, which are classes that collect the endpoints exposed by the application.
│       │   │
│       │   └───signatures ==> utilities module
│       │       │   __init__.py 
│       │       │   requests.py   ==> module containing the pydantic BaseModels representing the requests for the endpoints exposed by the application.
│       │       └── responses.py  ==> module containing the pydantic BaseModels representing the responses provided by the endpoints exposed by the application.
│       │
│       └───logic
│           │   __init__.py  
│           │   entities.py    ==> module containing pydantic BaseModels for this application
│           │   exceptions.py  ==> module containing exceptions specific for this application.
│           │   services.py    ==> module containing the services (a.k.a. 'ports') of this application.
│           └── usecases.py    ==> module containing the abstractions, a.k.a. 'usecases', of the services of this application.
│   
├───bin ==> folder that will contain executable files
│
├───config ==> folder to collect app an logging configurations. In this folder shoudl be added further configuration files for dev and prod environments that will overwrite defaults, where required.
│   │   defaults.yaml         ==> defaults app configurations, to be extendend and edited as necessary
│   │   app.dev.yaml          ==> app configurations for development environment, to be extendend and edited as necessary
│   │   app.prod.yaml         ==> app configurations for production environment, to be extendend and edited as necessary
│   │   logConfDefaults.yaml  ==> defaults logging configurations
│   │   logConfDev.yaml       ==> logging configurations for development environment
│   └── logConfProd.yaml      ==> logging configurations for production environment
│   
├───requirements
│   │   requirements.txt      ==> application's closed requirements, as built by make reqs
│   └── requirements_dev.txt  ==> development environment's open requirements, as built by make reqs_dev
│   
├───sphinx ==> sphinx documentation folder containig varius files that will be used to compile code documentations
│   └── ... 
│   
├───tests ==> unit-tests module
│   └── __init__.py
│
└───api_keys ==> folder, ignored by git, to contain secrets
```

## GNU Make
This project uses [GNU make](https://www.gnu.org/software/make/) to orchestrate of some complex common actions. 
In particular the processes of compiling and installing requirements, running checks (static typing, linting, unittests, etc), 
compiling documentation, building a docker image and running it are streamlined within the Makefile and can be run using 
simple make commands.
To get a complete list of available make commands the user can simply type `make help` but the most relevant are:
* ``make setup``, to setup the minimal environment required for the application
* ``make setup_dev``, to setup the full development environment (i.e. including dependencies required for quality checks)
* ``make install``, to install the package with minimal requirements
* ``make install_dev``, to install the package with development requirements
* ``make checks``, to check formatting, linting, static typing and running unit-tests
* ``make changelogs``, to compile changelogs
* ``make build``, to build Docker image
* ``make run``, to build run Docker image
* ``make tag``, to create and push on origin a 'development' git tag with current app version. This will trigger [CD workflow](#CD-workflow).

## Tools for the project
All the configurations for the following tools are contained in ``pyproject.toml`` file.

### Requirements tracking
This project uses [uv](https://github.com/astral-sh/uv) to handle requirements. 
In particular, we define the package requirements, optional extra requirements and dependency groups in the `pyproject.toml` 
configuration file, and we use `uv pip compile` to compile the complete sets of closed requirements for the package and the
development environment (to be used only for reference!) and save them in the `requirements` folder as ``requirements.txt`` 
and ``requirements_dev.txt`` files.

### Coding style
This project uses [ruff](https://github.com/astral-sh/ruff) for formatting and enforcing a coding style.
Run ``make format`` to reformat all source code and tests files to adhere to PEP8 standards.

### Code analysis
This project uses [ruff](https://github.com/astral-sh/ruff) for static code analysis and [pydoclint](https://github.com/jsh9/pydoclint) to analyse docstrings.
Default configurations for ``ruff`` are the ones suggested by the ``ruff`` developers themselves while the default docstring format checked with ``pydoclint`` is ``'sphinx'``. 
Note that default  ``ruff`` configurations include also the ``S`` rule set that check for security issues.
Run ``make lint`` to analyse all source code and tests files.

### Static type checking
This project uses [mypy](https://github.com/python/mypy) for static type checking.
The default configurations for this tool are quite strict. In particular we set the following flags to true:
* explicit_package_bases: This flag tells mypy that top-level packages will be based in either the current directory, or a member of the MYPYPATH environment variable or mypy_path config option. This option is only useful in the absence of __init__.py. See [Mapping file paths](https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-paths-to-modules) to modules for details.
* namespace_packages: Enables import discovery of namespace packages (see [PEP 420](https://peps.python.org/pep-0420/)). In particular, this prevents discovery of packages that don’t have an __init__.py (or __init__.pyi) file. his flag affects how mypy finds modules and packages explicitly passed on the command line. It also affects how mypy determines fully qualified module names for files passed on the command line. See [Mapping file paths to modules](https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-paths-to-modules) for details.
* disallow_untyped_calls: Disallows calling functions without type annotations from functions with type annotations.
* disallow_untyped_defs: Disallows defining functions without type annotations or with incomplete type annotations.
* disallow_untyped_decorators: Reports an error whenever a function with type annotations is decorated with a decorator without annotations.
* disallow_any_generics: Disallows usage of generic types that do not specify explicit type parameters.
* disallow_subclassing_any: Disallows subclassing a value of type Any.
* warn_unused_configs: Warns about per-module sections in the config file that do not match any files processed when invoking mypy.
* warn_redundant_casts: Warns about casting an expression to its inferred type.
* warn_unused_ignores: Warns about unneeded # type: ignore comments.
* warn_return_any: Shows a warning when returning a value with type Any from a function declared with a non- Any return type.
* implicit_reexport: By default, imported values to a module are treated as exported and mypy allows other modules to import them. When false, mypy will not re-export unless the item is imported using from-as or is included in __all__. Note that mypy treats stub files as if this is always disabled.
* strict_equality: Prohibit equality checks, identity checks, and container checks between non-overlapping types.
Run ``make mypy`` to analyse all source code and tests files.

### License compatibility checks
This project uses [`licensecheck`](https://github.com/FHPythonUtils/LicenseCheck) to check compatibility between the license of this package and the ones of its depencencies.

### Unit-testing
This project uses [`pytest`](https://docs.pytest.org/en/stable/) to run unit-tests.
Default configurations searches for tests under the `tests` folder. 
It uses the `pytest-cov` plugin to measure the test coverage of the source code and the `pytest-xdist` plugin to run 
unit-tests in parallel (parallelization is done on test file using the default number of process for the calling system).
Run `make tests` to run all tests.

### Documentation
This project uses [`Sphinx`](https://www.sphinx-doc.org/en/master/) as a tool to create documentation. 
Run `make docs` to automatically build documentation in html format.
There is a Github workflow setup to publish documentation on the repo's Github Page at every push on `main` branch. 
To let this action run smoothly there must exist the `gh_pages`branch and the Github Page must be manually setted (from
github repo web interface > Settings > Pages) to use `gh_pages` as source branch and `/root` as source folder. 
Since this action requires a GITHUB_TOKEN, for its first run in the repo it will be necessary to follow the steps 
detailed [here]( https://github.com/peaceiris/actions-gh-pages#%EF%B8%8F-first-deployment-with-github_token) to make the action run fine from then on.

### Semantic Versioning
This project automatically applies semantic versioning based on tags following PEP440 using [`setuptools_scm`](https://github.com/pypa/setuptools_scm).

### Changelog generation
This project uses [`git-cliff`](https://github.com/orhun/git-cliff) to automatically generate changelogs. 
This tool is configured to take into account commit messages following the [conventional-commits](https://www.conventionalcommits.org/) standard.

## Git hooks
This project uses [`pre-commit`](https://github.com/pre-commit/pre-commit), as configured in `.pre-commit-config.yaml` file, to run git hooks at different stages.
In particular, we define the following hooks:
* checks: runs the local ``make checks`` command at pre-push stage.
* detect-secrets: uses [`detect-secrets`](https://github.com/Yelp/detect-secrets) to automatically search for secrets versioned in the current diff. It runs at pre-commit stage.
* commitlint: uses [`commitlint`](https://github.com/alessandrojcm/commitlint-pre-commit-hook) to check, at commit-msg stage, that commit message follows the [conventional-commits](https://www.conventionalcommits.org/) standard as configured in the `.commitlintrc.yaml` file. 
Specifically:
  - Commits must start with a type, such as "chore", "fix", "feat", "doc", "perf", "refactor", "style" or "test" followed by an OPTIONAL scope in parentheses, an OPTIONAL exclamation mark "!" to indicate non-backward-compatible changes (a.k.a. "Breaking Changes"), and a colon and a space (": ") which are both REQUIRED.
  - The "chore" type should be used when a commit represents an insignificant internal change (i.e., a change that does not directly impact how the user interacts with the application).
  - The "fix" type should be used when a commit represents a bugfix to the application (i.e., a change that either doesn't directly impact the user or corrects an error).
  - The "feat" type should be used when a commit adds a new feature to the application (i.e., an addition that directly impacts how the user interacts with the application).
  - A scope CAN be included in parentheses after the type. A scope typically consists of a single word indicating a code module affected by the commit or a Jira task identifier, e.g., "fix(parser):" or "fix(prjaiaas-134)".
  - The description MUST immediately follow the colon-space ": " after the prefix containing the type and, optionally, the scope. It MUST end with a period (dot). The description is a brief summary in ENGLISH of the code changes. E.g., "fix: solved parsing issues with multiple spaces in strings.". 
  - A longer commit body CAN be provided after the description to give additional information about the changes made to the code (particularly useful for very large commits). The body MUST start with a blank line after the description and MUST be written in ENGLISH. 
  - The commit body is in free form and MAY consist of any number of paragraphs separated by a new line. 
  - One or more footers CAN be provided on a blank line after the commit body. Each footer MUST consist of a single-word token followed by ": " or " #", followed by a string (this is inspired by the git trailer convention). 
  - A footer token MUST use the dash ("-") character instead of spaces, for example Acked-by (this helps differentiate the footer section from a multi-paragraph commit body). An exception is made for "BREAKING CHANGE", which MAY also be used as a footer token. 
  - A footer's value CAN contain spaces and newlines: the automatic parsing of a footer MUST stop when the next valid token/separator pair of the next footer is observed. 
  - Breaking changes MUST be indicated in the commit's type/scope prefix, or as a specific footer entry. 
  - If included as a footer, Breaking Changes MUST start with the token "BREAKING CHANGE: " followed by a description of the change, e.g., BREAKING CHANGE: environment variables now have precedence over configuration files. 
  - If included in the prefix with the type and, optionally, scope, breaking changes MUST be indicated with an exclamation mark "!" immediately before the colon ":" that concludes the prefix. If "!" is used, "BREAKING CHANGE: " CAN be omitted from the footer section, and the description MUST be used to describe the breaking change.
  - "BREAKING-CHANGE" MUST be synonymous with "BREAKING CHANGE" when used as a footer token.
## Continuous Integration and Continuous Delivery
CI/CD workflows can be performed with either GithubActions of Circleci but, in any case, the same steps are performed.

### CI workflow
This workflow is activated only by commits on an open Pull Request and consists of the following steps:
1. Check the application, by running ``make checks`` on checked-out code.
2. Check code quality using Sonarqube.

### CD workflow
This workflow is activated only when a tag with name respecting `v*.*.*` regex is pushed on `origin` repository and consists of the following steps:
1. Check the application, by running ``make checks`` on checked-out code.
2. Check code quality using Sonarqube.
3. Build docker container, test it for vulnerabilities with Trivy, push it to container registry and deploy it on CloudRun in deploy GPC project
4. If the tag matches exactly `/^v(?!\.)(\d+(\.\d+)+)(?![\d\.])$/` (i.e. if it is a "production" tag): Build docker container, test it for vulnerabilities with Trivy, push it to container registry and deploy it on CloudRun in production GPC project

   